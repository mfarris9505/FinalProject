---
title: "Final_Project"
author: "Matthew Farris"
date: "December 12, 2015"
output: 
  html_document:
    toc: true
    theme: united
---

## Introduction 
  As we know, these are tulmutous times for our society. With recent events, Police Brutatlity has been on the fore-front of everyones minds. Such has created the need for better oversight, and more realistic expectations for our police force. As such, many people and journalists have taken it upon themselves to document police violence. One site in particular is ["The Counted"](http://www.theguardian.com/us-news/ng-interactive/2015/jun/01/the-counted-police-killings-us-database) This site uses crowdsourced data to identify all the fatalities by police officers in 2015. This site creates an instant source for oversight and documentation, something that is a necessity in our world today. By creating this site, we are no longer relying on the government to provide the data we need to understand trends in our society, but documenting the data ourselves. 
  
  The purpose of this project isn't just about the findings, but as a stepping stone to better understand how we the importance of up to date data. This project was done in conjunction with another project that analyzed just a single dataset. That project focused on poverty data, and can be seen [here](http://rpubs.com/mfarris9505/133528). It is my hope with the project to better understand how to extract data from the web and elsewhere, and combine it into a unique dataset. 
  
  The focus of this project will take data from 3 separate sources, The Counted Data set, population data by state, and the most recently available data for Police Force size to determine if there is some link between Police Size and the number of Police Fatalities. We hope to answer the question, do larger Police Forces result in an increased number of deaths by police?   

## Obtaining Data
```{r echo=FALSE}
library(knitr)
library(RCurl)
library(ggplot2)
library(ggthemes)
library(Rmisc)
library(jsonlite)
library(stringr)
library(tidyr)
library(plyr)
library(dplyr)
library(rvest)

```

### Counted Data Extraction

The first data set we will be pulling from is the Counted Data. Currently, the data source is available via download to CSV. The data file was uploaded to a github page and be extracted as follows: 

```{r}
url_data <- getURL("https://raw.githubusercontent.com/mfarris9505/FinalProject/master/the-counted.csv")

count_data <- read.csv(textConnection(url_data))
#Removing some data that is unnecessary for our purpose here to streamline the appearance 
count_data <- count_data[-c(1,3:10,13)]
head(count_data)
```

### Population Data Extraction
To create a rate, we needed the general population per state. To do this, data was pulled from the American Community Survey API from Census.gov. A brief overview of the census data can be seen by viewing this site(aside- this is the most cumbersome API I have seen to date, it was not very well organized or documented, I think its in Beta at the moment, and it seems they removed all their old documentation). [Discovery Tool Census Data](http://www.census.gov/data/developers/updates/new-discovery-tool.html)  

This specific data source uses 5 years of data to best estimate population data. This data is from 2014, which is the most up to date population that I could find after navigating the site. Using this API, a list was pulled using JSON: 

```{r}
url<- "http://api.census.gov/data/2014/acs5?get=NAME,B01001_001E&for=state:*"

pop2014_data <- fromJSON(url)
pop2014_data <- data.frame(pop2014_data)
pop2014_data <- pop2014_data[-1,]
#Remove excess column
pop2014_data$X3 <- NULL
names(pop2014_data) <- c("State","Population")

head(pop2014_data)
```

### Police Force Data Extraction
For the other data source we will be using, we will be performing a simple webscrape, extracting a web table. This data is actually recent FBI data, which is available in CSV form. However, the data in the table on the webpage is in the version which will facilitate cleaning. (The CSV surprisingly is more difficult to transform). The table can be seen [here](http://www.governing.com/gov-data/safety-justice/police-officers-per-capita-rates-employment-for-city-departments.html). This data will cleaned and then combined to create a single rate per state for the two categories to create a comparison. 

```{r}
police_url <- "http://www.governing.com/gov-data/safety-justice/police-officers-per-capita-rates-employment-for-city-departments.html"

police_data <- police_url %>% 
  read_html() %>% 
  html_nodes(xpath ='//*[@id="inputdata"]') %>% 
  html_table()

police_data <- data.frame(police_data)
# Removing some Excess data (Working with rates not totals)
police_data <- police_data[-c(2:4,6)]
head(police_data)
```

This data set was choosen as it best normalizes the data. This data includes the rate of each city per 10k population. As this has more than one city per state (for the most part), an average police size can be found by taking the average of these individual rates from each city.This should give an approximation of the total state police officer per citizen. This is making alot of assumptions, and is creating an inherent bias on large populations, but this can't be prevented, as the data is limited to only large cities.  

### State Abbreviations

One other piece of data that I needed to obtain was States Abbreviations. If you look, one of data sources (Population), listed only the states names. This would be problematice in the future, so we uploaded a states Abbr data frame, with another simple webscrape. 

```{r}
abbr_url <- "http://www.50states.com/abbreviations.htm#.Vm3q9UorKUk"

abbr_data <- abbr_url %>% 
  read_html() %>% 
  html_nodes(xpath ='//*[@id="content"]/div[1]/table') %>% 
  html_table()

abbr_data <- data.frame(abbr_data)
names(abbr_data) <- c("State", "Abbr")
```


## Data Scrubbing and Transformation

### The Counted Data

For our purposes here we need to transform this data in a single data frame with the rate of police fatalities per 100k population. To do this we need to use some summary functions. 

```{r}
count_data$count <- 1

counted_data <- count_data %>%
  group_by(state) %>%
  summarise(n()) 
names(counted_data) <- c("State", "Fatalities")
```


Now that we have a summary of the data, we must combine the population data to create a rate. This can be done simply using the join function on the plyr package. But furst we have to convert the states to abbreviations. This can be done several ways, but the easiest would be to use the same join function that will match the state columns. We are using a left join here as we only need the data from the left table. 

```{r}
pop2014_data <- join(pop2014_data,abbr_data, by = "State")
#Removing the Full State Name
pop2014_data$State <- NULL
names(pop2014_data) <- c("Population", "State")
head(pop2014_data)
```

Now the population can be combined with to the Counted data. We will be using another left join, as the COunted Data does not have a complete list of states. This is because some data is not represented here (ie. Puerto Rico and Vermont which had no deaths this year). The rate per Million for the data was also calculated in this step.   

```{r}
counted_combined <- join(counted_data, pop2014_data, by = "State")
counted_combined$Fatalities <- as.numeric(as.character(counted_combined$Fatalities))
counted_combined$Population <- as.numeric(as.character(counted_combined$Population))

counted_combined <- mutate(counted_combined, Fatal_Rate_Per_Mil = (Fatalities/Population * 1000000))

head(counted_combined)
```

### Police Force Data 

We need to complete a similar step for the Police Data. To start we need to collect the state data. This must be done using the tidyr separate function:

```{r}
police_data <- police_data %>% separate(City, c("City", "State"), ",")
police_data$City <- NULL
police_data$State <-str_trim(police_data$State)
head(police_data)
```

We can now transfrom this into similar data. As we are using rates here. It is important to take the average rather that the sum when calculating the states data: 

```{r}
police_final <- police_data %>%
  group_by(State) %>%
  summarise(mean(Officers.per.10K.residents)) 
names(police_final) <- c("State", "Officer_Per_10k" )

```

Now we can effectively combine both into a single dataset that we can use for modeling. 

```{r}
counted_combined$State <- as.character(counted_combined$State)
counted_combined$State <- factor(counted_combined$State)
police_final$State <- as.character(police_final$State)
police_final$State <- factor(police_final$State)

counted_combined <- join(counted_combined, police_final, by = "State")

head(counted_combined)
```

## Explore and Visualize

Now that the data is condensed and collated, it is time to start exploring what we have created. The best way to do this would be to provide some summary statistics: 

```{r}
summary(counted_combined)
```


After reviewing the summary statistics, it is important to visualize the data to see if we have any discrepancies, to see if the data approaches a normal trend, etc. Boxplot and histograms are an appropriate way to better "see" the data:  


```{r echo = FALSE}
p1 <- ggplot(counted_combined, aes(1, Officer_Per_10k)) +
  geom_boxplot()+
  theme_stata() + 
  scale_colour_stata() +
  ggtitle("Police Size Boxplot")

p2 <- ggplot(counted_combined, aes(Officer_Per_10k)) +
  geom_histogram(binwidth = 5)+
  theme_stata() + 
  scale_colour_stata() +
  ggtitle("Police Size Histogram")

p3 <- ggplot(counted_combined, aes(1, Fatal_Rate_Per_Mil)) +
  geom_boxplot()+
  theme_stata() + 
  scale_colour_stata()+
  ggtitle("Fatalities Boxplot")

p4 <- ggplot(counted_combined, aes(Fatal_Rate_Per_Mil)) +
  geom_histogram(binwidth = .3)+
  theme_stata() + 
  scale_colour_stata() +
   ggtitle("Fatalities Histogram")

multiplot(p1,p2,p3,p4, cols = 2)
```

From what we can see here, is we have some potential Outliers. An examination of outliers and how to remove them will be discussed in a later course. However, for our purposes here, the one Outlier that is causing much concern is the DC data, particularly in terms of the higher number of police officers per citizen. As DC is not a State, and represents a rather heavily policed area (particularly because of the importance of certain individuals who reside there) I believe it is warranted to remove this particular point. As we want to make a comparsion to Police size, this one would create an inherent bias. Again, not too familiar with the process of eliminating outliers, but these reasons seem to justify it's removal. With this in mind we can start our modeling process.

## Modelling
For our process here, we determined that linear regression would likely be the best model, and the one I am currently most familiar with. Using linear regression. It is best to start with a plot of the two variables to determine if police size in any way influences the fatalities by officers. We will perform a simple linear regression model with a 95% confidence. Our null hypothesis is that slope or $\beta$ = 0, and our alternate is that $\beta$ does not equal 0.

```{r}
counted_combined <- counted_combined[-8,]
ggplot(counted_combined, aes(Officer_Per_10k, Fatal_Rate_Per_Mil)) +
  geom_point()+
  geom_smooth(method="lm") +
  theme_stata() + 
  scale_colour_stata()
```

As we can see from the graph, the linear model shows that there is an apparent decrease in fatalities with larger sized police forces. However, it is good to note here, that the confidence region shows that their is not much signifcance in this data, as the true slope with a 95% probabilty could be also be positive. This can be further seen by reviewing the the linear model function. 

```{r}
counted_lm <- lm(Fatal_Rate_Per_Mil ~ Officer_Per_10k, data = counted_combined)
summary(counted_lm)
```

Again, we can see from our summary that the model is not statisitcally significant. The p-value in the model is much too high. We fail to reject the null hypothesis. 

Even though we failed to reject the null, it is important to deterling if the linear model is valid. To do this we must check to see if meets certain criteria. First, the model must be linear, which we can see from the graph. Also, plotting the residuals vs police force will give us an idea of linearity. This can be seen in Plot 1 below. Furthermore, a plot of the residual histogram should approach normal, that can be seen in Plot 2

```{r}
q1 <- ggplot(data = NULL, aes(x=counted_combined$Officer_Per_10k, y=counted_lm$residuals)) +
  geom_point() +
  geom_abline(intercept=0, slope=0) +
  ggtitle("Residual Plot 1")

q2 <- ggplot(data = counted_lm, aes(Officer_Per_10k)) +
  geom_histogram() +
  ggtitle("Histogram Plot 2")

multiplot(q1,q2, cols = 2)
```

From what I can see, the linearity doesn't past muster, there is not a consistent distribution, we can see some patterns developing. This could be an indication that there is some other model (that I am unfamiliar with) which may be better suited. The residuals, however, do appear to be normally distributed, with a slight skew. From what we can see from both the p-value and the tests, is that the linear model is not a good fit for the data that we have. It would be best to use a more appropriate modelling method.  

## Interpret and Conclusion 

  From the data presented above, we can see that we failed to reject our null, and our data here shows no apparent linear relationship between Police Size and Police Fatalities. However, this isn't a clear indidication that there is no pattern at all. The fact is there was some interesting residual data could indicate that there is another more acurate representation. Furthermore, the data itself was obtained on an "best-estimate." The state data was calculated from the cities with the most populations. This would like unfairly bias the data towards big cities. As there was no cross-check to determine if police fatalities were mostly from densely populated cities, we can safely say that the data may not be an accurate representation of the population in focus. Further study would be best done using city data. However, as the number of police fatalities is dispersed over a large population, city data would only result in a small number. Furthermore, there was not a one to one relation between our two sources, and some city data would not be represented. This was one of the reasons why I choose to combine the data across the state level, as it allowed for a greater one to one realtionship. As you can see, this choice may have limited any patterns amoung the data source.  

  The take away from this project is this, our data is getting better. With the prevalence of modern distribution of information, including but not limited to crowdsourcing, we now have better way to identify problems. One such problem is the number of deaths by police officers. The value of having an update source will allow for a more real-time analysis that can better prevent future deaths. However, this real-time data is not readily available every where. Some data is still exceeding difficult to update. Furthermore, certain goverment data is still significantly behind. Hopefully, one day data will come closer and closer to real-time. 
## Sources

* Kenan Davis, Rich Harris, Nadja Popovich and Kenton Powell, ["The Counted"](http://www.theguardian.com/us-news/ng-interactive/2015/jun/01/the-counted-police-killings-us-database), 2015

* "State Abbreviations" [50States](http://www.50states.com/abbreviations.htm#.Vm5IdUorKUl), 2015

* American Community Survey 5 Year Data (2010 - 2014) [Census.gov](http://www.census.gov/data/developers/data-sets/acs-survey-5-year-data.html), 2015

* [Governing](http://www.governing.com/gov-data/safety-justice/police-officers-per-capita-rates-employment-for-city-departments.html), 2015